{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 99\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import expit\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from db_queries import get_location_metadata\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "INPUT_DIR = '/ihme/covid-19/deaths/mobility_inputs/2020_04_14'\n",
    "\n",
    "MOBILITY_FILE = f'{INPUT_DIR}/google_mobility_with_locids.csv'\n",
    "POP_DENS_FILE = f'{INPUT_DIR}/pop_density.csv'\n",
    "# MOBILITY_POP_DENS_DIR = '/home/j/WORK/01_covariates/02_inputs/population_density/outputs_2020_covid_mob_20200328'\n",
    "PEAK_FILE = f'{INPUT_DIR}/final_peak_locs_04_14.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get dates from deaths file (will want to redo this with a non-model dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "data_date = pd.Timestamp('2020-04-13')\n",
    "n_days = 19\n",
    "\n",
    "# get peak day\n",
    "peak_day_df = pd.read_csv(PEAK_FILE)\n",
    "peak_day_df = peak_day_df.loc[peak_day_df['Country/Region'] != 'China']\n",
    "\n",
    "# drop CO (don't believe), Puglia + Aragon (peak is in future)\n",
    "peak_day_df = peak_day_df.loc[~peak_day_df['Location'].isin(['Colorado', 'Puglia', 'Aragon'])]\n",
    "\n",
    "# # filter to admin1s?\n",
    "# peak_day_df = peak_day_df.loc[peak_day_df['location'].str.endswith(('Germany', 'Italy', 'Spain', 'United States of America'))]\n",
    "\n",
    "# get `n` days before peak\n",
    "peak_day_df['Peak date'] = pd.to_datetime(peak_day_df['peak_date'])\n",
    "peak_day_df['R0 date'] = peak_day_df['Peak date'].apply(lambda x: x - timedelta(days=n_days))\n",
    "print(f\"{', '.join(peak_day_df['Country/Region'].unique().tolist())}\")\n",
    "peak_day_df = peak_day_df.loc[~peak_day_df['Peak date'].isin([data_date - timedelta(days=i) for i in range(3)])]\n",
    "peak_day_df = peak_day_df[['location_id', 'Peak date', 'R0 date']].reset_index(drop=True)\n",
    "peak_day_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in and format Google mobility data\n",
    "#### MANUALLY SUBSETTING ISO-2 BASED ON ORIGINAL LIST ABOVE (OK IF WE HAVE MORE COUNTRIES THAN THAT LIST, BUT NOT FEWER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_df = pd.read_csv(MOBILITY_FILE)\n",
    "# google_vars = [i for i in mob_df.columns if i.startswith('google_')]\n",
    "google_vars = ['google_retail_and_recreation', 'google_transit_stations', 'google_workplaces']\n",
    "mob_df['Date'] = pd.to_datetime(mob_df['date'])\n",
    "mob_df = mob_df.loc[~mob_df['location_id'].isnull()]\n",
    "mob_df['location_id'] = mob_df['location_id'].astype(int)\n",
    "mob_df['avg_mobility'] = mob_df[google_vars].mean(axis=1)\n",
    "mob_df['Location'] = mob_df['location_name']\n",
    "mob_df = mob_df[['location_id', 'Location', 'Date', 'avg_mobility']]\n",
    "# print(f\"No matches for {'; '.join([i for i in peak_day_df['Location'].unique().tolist() if i not in mob_df['Location'].unique()])}\")\n",
    "mob_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_dens_df = pd.read_csv(POP_DENS_FILE)\n",
    "\n",
    "# just take >= 1000\n",
    "pop_dens_df.loc[\n",
    "    pop_dens_df['pop_density'].isin(['1000-2500 ppl/sqkm', '2500-5000 ppl/sqkm', '5000-10000 ppl/sqkm', '10000-20000 ppl/sqkm', '>=20000 ppl/sqkm']),\n",
    "    'pop_density'\n",
    "] = '>=1000 ppl/sqkm'\n",
    "pop_dens_df = pop_dens_df.loc[pop_dens_df['pop_density'] == '>=1000 ppl/sqkm']\n",
    "pop_dens_df = pop_dens_df.groupby(['location_id', 'location_name', 'pop_density'], as_index=False)['pop_proportion'].sum()\n",
    "\n",
    "# reshape\n",
    "pop_dens_df = pd.pivot_table(pop_dens_df, index=['location_id', 'location_name'], \n",
    "                             columns='pop_density', values='pop_proportion').reset_index()\n",
    "pop_dens_df = pop_dens_df.rename(index=str, columns={'location_name':'Location'})\n",
    "pop_dens_vars = [i for i in pop_dens_df.columns if i not in ['location_id', 'Location']]\n",
    "pop_dens_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get interacted pop dens * mobility variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction_files = {\n",
    "#     # 'residential': 'all_outputs_2020_rs.csv', \n",
    "#     'retail_rec': 'all_outputs_2020_rr.csv', \n",
    "#     'transit': 'all_outputs_2020_ts.csv', \n",
    "#     'work': 'all_outputs_2020_wk.csv'\n",
    "# }\n",
    "\n",
    "# int_dfs = []\n",
    "# for google_metric, file_name in interaction_files.items():\n",
    "#     int_df = pd.read_csv(f'{MOBILITY_POP_DENS_DIR}/{file_name}')\n",
    "#     int_df.loc[\n",
    "#         int_df['pop_density'].isin(['<150 ppl/sqkm', '150-300 ppl/sqkm', '300-500 ppl/sqkm']),\n",
    "#         'pop_density'\n",
    "#     ] = '<500 ppl/sqkm'\n",
    "#     int_df.loc[\n",
    "#         int_df['pop_density'].isin(['500-1000 ppl/sqkm', '1000-2500 ppl/sqkm', '2500-5000 ppl/sqkm']),\n",
    "#         'pop_density'\n",
    "#     ] = '500-5000 ppl/sqkm'\n",
    "#     int_df.loc[\n",
    "#         int_df['pop_density'].isin(['10000-20000 ppl/sqkm', '>=20000 ppl/sqkm']),\n",
    "#         'pop_density'\n",
    "#     ] = '>=10000 ppl/sqkm'\n",
    "#     int_df['Date'] = pd.to_datetime(int_df['date'])\n",
    "#     int_df = int_df.groupby(['location_id', 'location_name', 'Date', 'pop_density'], as_index=False)['pop_proportion'].sum()\n",
    "#     int_df = pd.pivot_table(int_df, index=['location_id', 'location_name', 'Date'], \n",
    "#                             columns='pop_density', values='pop_proportion').reset_index()\n",
    "#     int_dfs.append(int_df)\n",
    "    \n",
    "# # combine dataframes\n",
    "# int_df = pd.concat(int_dfs)\n",
    "# int_df = int_df.groupby(['location_id', 'location_name', 'Date'], as_index=False)[pop_dens_vars].mean()\n",
    "# int_df = int_df.rename(index=str, columns={'location_name':'Location'})\n",
    "# int_vars = [i for i in int_df.columns if i not in ['location_id', 'Location', 'Date']]\n",
    "\n",
    "# # add iso2 placeholder just for US\n",
    "# loc_df = get_location_metadata(location_set_id=35, gbd_round_id=6)\n",
    "# int_df = int_df.merge(loc_df[['location_id', 'parent_id']])\n",
    "# int_df['iso2'] = 'XX'\n",
    "# int_df.loc[int_df['parent_id'] == 102, 'iso2'] = 'US'\n",
    "# int_df.loc[int_df['parent_id'] == 81, 'iso2'] = 'DE'\n",
    "# del int_df['parent_id']\n",
    "# int_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flag date since closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closure_vars = [\n",
    "    'Any Gathering Restrictions', 'People instructed to stay at home', \n",
    "    'Educational facilities closed',\n",
    "    'Any Business Closures', 'Non-essential services closed (i.e., bars/restaurants)'\n",
    "]  \n",
    "closure_df = pd.read_excel('/ihme/covid-19/snapshot-data/best/covid_onedrive/Decrees for Closures/closure_criteria_sheet.xlsx')\n",
    "closure_df = closure_df.loc[(~closure_df['merge_name'].isnull()) & (~closure_df['location_id'].isnull())]\n",
    "closure_df = closure_df[['location_id'] + closure_vars]\n",
    "for closure_var in closure_vars:\n",
    "    closure_df[closure_var] = closure_df[closure_var].apply(\n",
    "        lambda x: datetime.strptime(x, '%d.%m.%Y') if isinstance(x, str) and x[0].isdigit() else np.nan\n",
    "    )\n",
    "\n",
    "# give partial credit\n",
    "closure_df.loc[closure_df['People instructed to stay at home'] == 1, \n",
    "               'Any Gathering Restrictions'] = closure_df['People instructed to stay at home']\n",
    "closure_df.loc[closure_df['Non-essential services closed (i.e., bars/restaurants)'] == 1, \n",
    "               'Any Business Closures'] = closure_df['Non-essential services closed (i.e., bars/restaurants)']\n",
    "\n",
    "# assign dummies\n",
    "closure_df['location_id'] = closure_df['location_id'].astype(int)\n",
    "closure_df = closure_df.merge(mob_df[['location_id', 'Date']])\n",
    "for closure_var in closure_vars:\n",
    "    closure_df[closure_var] = closure_df.apply(lambda x: 1 if x['Date'] >= x[closure_var] else 0, axis=1)\n",
    "closure_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get model dataset for separate pop density and Google metric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine datasets\n",
    "def prepare_model_inputs(df_list, indep_vars):\n",
    "    model_df = reduce(lambda x, y: pd.merge(x, y, how='outer'), df_list)\n",
    "    model_df = model_df.loc[~model_df[indep_vars].isnull().any(axis=1)]\n",
    "    model_df['location_id'] = model_df['location_id'].astype(int)\n",
    "\n",
    "    # fill in our variable\n",
    "    model_df['R0<=1'] = 0\n",
    "    model_df.loc[model_df['Date'] >= model_df['R0 date'], 'R0<=1'] = 1\n",
    "\n",
    "    # add dummy for US/non-US and Germany/non-Germany\n",
    "    loc_df = get_location_metadata(location_set_id=111, location_set_version_id=630)\n",
    "    us_loc_ids = loc_df.loc[loc_df['parent_id'] == 102, 'location_id'].to_list()\n",
    "    deu_loc_ids = loc_df.loc[loc_df['parent_id'] == 81, 'location_id'].to_list()\n",
    "    #\n",
    "    model_df['US'] = 0\n",
    "    model_df.loc[model_df['location_id'].isin(us_loc_ids), 'US'] = 1\n",
    "    #\n",
    "    model_df['DE'] = 0\n",
    "    model_df.loc[model_df['location_id'].isin(deu_loc_ids), 'DE'] = 1\n",
    "\n",
    "    # subset out prediction data\n",
    "    pred_df = model_df.copy()\n",
    "\n",
    "    # drop ANY nulls for actual model data\n",
    "    model_df = model_df.loc[~model_df.isnull().any(axis=1)]\n",
    "    model_locs = model_df['location_id'].unique()\n",
    "\n",
    "    # # split off test locs\n",
    "    # np.random.seed(15243)\n",
    "    # test_locs_idx = np.random.choice(model_locs.size, n, replace=False)\n",
    "    # train_locs = np.delete(model_locs, test_locs_idx, 0)\n",
    "    # # train_locs = np.array([i for i in train_locs if i != 541])\n",
    "    # test_locs = model_locs[test_locs_idx]\n",
    "    # # test_locs = np.array([541] + [i for i in test_locs if i != 541])\n",
    "    # train_df = model_df.loc[model_df['location_id'].isin(train_locs)]\n",
    "    # test_df = model_df.loc[model_df['location_id'].isin(test_locs)]\n",
    "    \n",
    "    return model_df, pred_df\n",
    "\n",
    "model_df, pred_df = prepare_model_inputs([peak_day_df, closure_df, mob_df], # pop_dens_df\n",
    "                                          closure_vars + ['avg_mobility'])  #  pop_dens_vars + \n",
    "# int_model_df, int_pred_df = prepare_model_inputs([peak_day_df, int_df], \n",
    "#                                                  int_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(15243)\n",
    "# same dep var\n",
    "lhs = 'R0<=1'\n",
    "\n",
    "## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ##\n",
    "# separate model\n",
    "rhs = closure_vars  # + ['avg_mobility']\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(\n",
    "    model_df[rhs], model_df[lhs]\n",
    ")\n",
    "pred_df['probability'] = logistic_model.predict_proba(pred_df[rhs])[:, 1]\n",
    "\n",
    "coef_dict = dict(zip(rhs, logistic_model.coef_[0].tolist()))\n",
    "## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ##\n",
    "\n",
    "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ##\n",
    "# ## interaction model\n",
    "# int_rhs = int_vars + ['US', 'DE']\n",
    "# logistic_model = LogisticRegression(max_iter=1000)\n",
    "# logistic_model.fit(\n",
    "#     int_model_df[int_rhs], int_model_df[lhs]\n",
    "# )\n",
    "# int_pred_df['probability'] = logistic_model.predict_proba(int_pred_df[int_rhs])[:, 1]\n",
    "# int_pred_df = int_pred_df.sort_values(['iso2', 'Location']).reset_index(drop=True)\n",
    "\n",
    "# int_coef_dict = dict(zip(int_rhs, logistic_model.coef_[0].tolist()))\n",
    "# ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## store and plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df = get_location_metadata(location_set_id=111, location_set_version_id=630)\n",
    "date_dfs = []\n",
    "with PdfPages(f'/ihme/homes/rmbarber/logistic_mobility_preds.pdf') as pdf:\n",
    "    for location_id in pred_df['location_id'].unique():\n",
    "        plot_df = pred_df.loc[pred_df['location_id'] == location_id].reset_index(drop=True)\n",
    "        location = plot_df['Location'][0]\n",
    "        if not plot_df['R0 date'].isnull().all():\n",
    "            true_date = plot_df['R0 date'][0]\n",
    "            prob_true_date = plot_df.loc[plot_df['Date'] == plot_df['R0 date'], 'probability'].item()\n",
    "        else:\n",
    "            true_date = None\n",
    "            prob_true_date = None\n",
    "        pred_cross_date_l = plot_df.loc[plot_df['probability'] >= 0.4, 'Date'].min()\n",
    "        pred_cross_date_m = plot_df.loc[plot_df['probability'] >= 0.5, 'Date'].min()\n",
    "        pred_cross_date_u = plot_df.loc[plot_df['probability'] >= 0.6, 'Date'].min()\n",
    "        \n",
    "        # if we have values, add to dataframe\n",
    "        if pred_cross_date_u is not pd.NaT and \\\n",
    "            (true_date is None or pred_cross_date_u > true_date - timedelta(days=4)):\n",
    "            date_dfs.append(\n",
    "                pd.DataFrame({\n",
    "                    'location_id':location_id,\n",
    "                    'Location':location,\n",
    "                    'p40_date':pred_cross_date_l,\n",
    "                    'p50_date':pred_cross_date_m,\n",
    "                    'p60_date':pred_cross_date_u\n",
    "                }, index=[0])\n",
    "            )\n",
    "            tag = ' - to use'\n",
    "        else:\n",
    "            tag = ' - not used'\n",
    "        \n",
    "        # plot\n",
    "        if location_id in loc_df['location_id'].to_list():\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(16.5, 8.5))\n",
    "\n",
    "            # # pop dens\n",
    "            # ax[0].bar(pop_dens_vars, \n",
    "            #         plot_df.loc[plot_df['Date'] == plot_df['Date'].min(), pop_dens_vars].values[0])\n",
    "            # # ax[0].set_xticks(rotation=60)\n",
    "\n",
    "            # google\n",
    "            ax[0].plot(plot_df['Date'],\n",
    "                       plot_df['avg_mobility'], color='black')  #  ({np.round(coef_dict[google_var], 2)})\n",
    "            if true_date is not None:\n",
    "                ax[0].axvline(true_date, color='black', linestyle='--', alpha=0.75)\n",
    "            if pred_cross_date_u is not pd.NaT:\n",
    "                ax[0].axvline(pred_cross_date_l, color='forestgreen', linestyle=':', alpha=0.75)\n",
    "                ax[0].axvline(pred_cross_date_m, color='dodgerblue', linestyle=':', alpha=0.75)\n",
    "                ax[0].axvline(pred_cross_date_u, color='firebrick', linestyle=':', alpha=0.75)\n",
    "\n",
    "            # model\n",
    "            ax[1].plot(plot_df['Date'],\n",
    "                       plot_df['probability'], color='black')\n",
    "            if true_date is not None:\n",
    "                ax[1].axvline(true_date, color='black', linestyle='--', alpha=0.75)\n",
    "            if pred_cross_date_u is not pd.NaT:\n",
    "                ax[1].axvline(pred_cross_date_l, color='forestgreen', linestyle=':', alpha=0.75)\n",
    "                ax[1].axvline(pred_cross_date_m, color='dodgerblue', linestyle=':', alpha=0.75)\n",
    "                ax[1].axvline(pred_cross_date_u, color='firebrick', linestyle=':', alpha=0.75)\n",
    "            ax[1].set_ylim(0, 1)\n",
    "            plt.suptitle(location + tag)\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()\n",
    "\n",
    "# combine prediction data\n",
    "date_df = pd.concat(date_dfs).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = date_df.loc[date_df['location_id'] != 35]  # drop the country Georgia for now\n",
    "date_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df.to_csv(f'{INPUT_DIR}/R0_dates.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
